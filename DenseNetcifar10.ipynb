{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of DenseNet - cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wVIx_KIigxPV",
        "outputId": "cb1b4c5b-56bd-4e54-d59e-d5bc29f77449",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import keras\n",
        "from tensorflow.keras.layers import Dense, Conv2D, BatchNormalization\n",
        "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D\n",
        "from tensorflow.keras.layers import Input, Flatten, Dropout\n",
        "from tensorflow.keras.layers import concatenate, Activation\n",
        "from tensorflow.keras.optimizers import RMSprop,Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler ,EarlyStopping\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UNHw6luQg3gc",
        "colab": {}
      },
      "source": [
        "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
        "# backend\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXxULY813x6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training parameters\n",
        "epochs = 100\n",
        "# network parameters\n",
        "num_classes = 10\n",
        "num_dense_blocks = 3\n",
        "growth_rate = 12\n",
        "depth = 100\n",
        "num_bottleneck_layers = (depth - 4) // (2 * num_dense_blocks)\n",
        "\n",
        "num_filters_bef_dense_block = 2 * growth_rate\n",
        "compression_factor = 0.5\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mB7o3zu1g6eT",
        "outputId": "530095c2-2382-4fa9-f842-6d69dd4176b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Load CIFAR10 Data\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "# input image dimensions\n",
        "input_shape = X_train.shape[1:]\n",
        "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=10/50, random_state=42)\n",
        "\n",
        "num_classes = 10\n",
        "# convert to one hot encoing \n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes) \n",
        "y_cv = tf.keras.utils.to_categorical(y_cv, num_classes)\n",
        "\n",
        "# convert from integers to floats\n",
        "train_norm = X_train.astype('float32')\n",
        "test_norm = X_test.astype('float32')\n",
        "cv_norm = X_cv.astype('float32')\n",
        "# normalize to range 0-1\n",
        "X_train = train_norm / 255.0\n",
        "X_test = test_norm / 255.0\n",
        "X_cv = cv_norm /255.0\t\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lAk_Mw_5-rn",
        "colab_type": "code",
        "outputId": "92e07c57-c344-47ce-a30d-4bb0dcb9145d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape , X_cv.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((40000, 32, 32, 3), (10000, 32, 32, 3))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVkpgHsc5-rp",
        "colab_type": "code",
        "outputId": "449e8217-150d-45b9-ce5c-7d6771f69ab3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ee-sge5Kg7vr",
        "outputId": "9a0a2039-5c29-4329-fb5b-aab7d821a450",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " # start model definition\n",
        "# densenet CNNs (composite function) are made of BN-ReLU-Conv2D\n",
        "\n",
        "inputs = Input(shape=input_shape)\n",
        "x = BatchNormalization()(inputs)\n",
        "x = Activation('relu')(x)\n",
        "x = Conv2D(num_filters_bef_dense_block,\n",
        "           kernel_size=3,\n",
        "           padding='same',\n",
        "           kernel_initializer='he_normal')(x)\n",
        "x = concatenate([inputs, x])\n",
        "\n",
        "# stack of dense blocks bridged by transition layers\n",
        "for i in range(num_dense_blocks):\n",
        "    # a dense block is a stack of bottleneck layers\n",
        "    for j in range(num_bottleneck_layers):\n",
        "        y = BatchNormalization()(x)\n",
        "        y = Activation('relu')(y)\n",
        "        y = Conv2D(4 * growth_rate,\n",
        "                   kernel_size=1,\n",
        "                   padding='same',\n",
        "                   kernel_initializer='he_normal')(y)\n",
        "        y = BatchNormalization()(y)\n",
        "        y = Activation('relu')(y)\n",
        "        y = Conv2D(growth_rate,\n",
        "                   kernel_size=3,\n",
        "                   padding='same',\n",
        "                   kernel_initializer='he_normal')(y)\n",
        "        x = concatenate([x, y])\n",
        "\n",
        "    # no transition layer after the last dense block\n",
        "    if i == num_dense_blocks - 1:\n",
        "        continue\n",
        "\n",
        "    # transition layer compresses num of feature maps and reduces the size by 2\n",
        "    num_filters_bef_dense_block += num_bottleneck_layers * growth_rate\n",
        "    num_filters_bef_dense_block = int(num_filters_bef_dense_block * compression_factor)\n",
        "    y = BatchNormalization()(x)\n",
        "    y = Conv2D(num_filters_bef_dense_block,\n",
        "               kernel_size=1,\n",
        "               padding='same',\n",
        "               kernel_initializer='he_normal')(y)\n",
        "    x = AveragePooling2D()(y)\n",
        "\n",
        "\n",
        "# add classifier on top\n",
        "# after average pooling, size of feature map is 1 x 1\n",
        "x = AveragePooling2D(pool_size=8)(x)\n",
        "x= Conv2D(num_classes,kernel_size=1,padding='valid',\n",
        "                kernel_initializer='he_normal',\n",
        "                activation='softmax')(x)\n",
        "outputs = Flatten()(x)    \n",
        "#outputs = Dense(num_classes,kernel_initializer='he_normal',activation='softmax')(x)         \n",
        "\n",
        "# instantiate and compile model\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(learning_rate=0.001),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 3)    12          input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 3)    0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 24)   672         activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 32, 32, 27)   0           input_1[0][0]                    \n",
            "                                                                 conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 27)   108         concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 27)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 48)   1344        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 48)   192         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 48)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 12)   5196        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 39)   0           concatenate[0][0]                \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 39)   156         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 39)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 48)   1920        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 48)   192         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 48)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 12)   5196        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 51)   0           concatenate_1[0][0]              \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 51)   204         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 51)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 48)   2496        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 48)   192         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 48)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 12)   5196        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 32, 32, 63)   0           concatenate_2[0][0]              \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 63)   252         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 63)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 48)   3072        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 48)   192         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 48)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 12)   5196        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 32, 32, 75)   0           concatenate_3[0][0]              \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 75)   300         concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 75)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 48)   3648        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 48)   192         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 48)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 12)   5196        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 32, 32, 87)   0           concatenate_4[0][0]              \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 32, 32, 87)   348         concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32, 32, 87)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 48)   4224        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 32, 32, 48)   192         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 32, 32, 48)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 12)   5196        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 32, 32, 99)   0           concatenate_5[0][0]              \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 32, 32, 99)   396         concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 32, 32, 99)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 48)   4800        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 32, 32, 48)   192         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 32, 32, 48)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 32, 32, 12)   5196        activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 32, 32, 111)  0           concatenate_6[0][0]              \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 32, 32, 111)  444         concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 32, 32, 111)  0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 32, 32, 48)   5376        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 32, 32, 48)   192         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 32, 32, 48)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 32, 32, 12)   5196        activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 32, 32, 123)  0           concatenate_7[0][0]              \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 32, 32, 123)  492         concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 32, 32, 123)  0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 32, 32, 48)   5952        activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 32, 32, 48)   192         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 32, 32, 48)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 32, 32, 12)   5196        activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 32, 32, 135)  0           concatenate_8[0][0]              \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 32, 32, 135)  540         concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 32, 32, 135)  0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 32, 32, 48)   6528        activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 32, 32, 48)   192         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 32, 32, 48)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 32, 32, 12)   5196        activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 32, 32, 147)  0           concatenate_9[0][0]              \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 32, 32, 147)  588         concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 32, 32, 147)  0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 32, 32, 48)   7104        activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 32, 32, 48)   192         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 32, 32, 48)   0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 32, 32, 12)   5196        activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 32, 32, 159)  0           concatenate_10[0][0]             \n",
            "                                                                 conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 32, 32, 159)  636         concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 32, 32, 159)  0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 32, 32, 48)   7680        activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 32, 32, 48)   192         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 32, 32, 48)   0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 32, 32, 12)   5196        activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 32, 32, 171)  0           concatenate_11[0][0]             \n",
            "                                                                 conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 32, 32, 171)  684         concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 32, 32, 171)  0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 32, 32, 48)   8256        activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 32, 32, 48)   192         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 32, 32, 48)   0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 32, 32, 12)   5196        activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 32, 32, 183)  0           concatenate_12[0][0]             \n",
            "                                                                 conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 32, 32, 183)  732         concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 32, 32, 183)  0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 32, 32, 48)   8832        activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 32, 32, 48)   192         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 32, 32, 48)   0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 32, 32, 12)   5196        activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 32, 32, 195)  0           concatenate_13[0][0]             \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 32, 32, 195)  780         concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 32, 32, 195)  0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 32, 32, 48)   9408        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 32, 32, 48)   192         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 32, 32, 48)   0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 32, 32, 12)   5196        activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 32, 32, 207)  0           concatenate_14[0][0]             \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 32, 32, 207)  828         concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 32, 32, 207)  0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 32, 32, 48)   9984        activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 32, 32, 48)   192         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 32, 32, 48)   0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 32, 32, 12)   5196        activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 32, 32, 219)  0           concatenate_15[0][0]             \n",
            "                                                                 conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 32, 32, 219)  876         concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 32, 32, 108)  23760       batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 16, 16, 108)  0           conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 16, 16, 108)  432         average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 16, 16, 108)  0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 16, 16, 48)   5232        activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 16, 16, 48)   192         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 16, 16, 48)   0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 16, 16, 12)   5196        activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 16, 16, 120)  0           average_pooling2d[0][0]          \n",
            "                                                                 conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 16, 16, 120)  480         concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 16, 16, 120)  0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 16, 16, 48)   5808        activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 16, 16, 48)   192         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 16, 16, 48)   0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 16, 16, 12)   5196        activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 16, 16, 132)  0           concatenate_17[0][0]             \n",
            "                                                                 conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 16, 16, 132)  528         concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 16, 16, 132)  0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 16, 16, 48)   6384        activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 16, 16, 48)   192         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 16, 16, 48)   0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 16, 16, 12)   5196        activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 16, 16, 144)  0           concatenate_18[0][0]             \n",
            "                                                                 conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 16, 16, 144)  576         concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 16, 16, 144)  0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 16, 16, 48)   6960        activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 16, 16, 48)   192         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 16, 16, 48)   0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 16, 16, 12)   5196        activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 16, 16, 156)  0           concatenate_19[0][0]             \n",
            "                                                                 conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 16, 16, 156)  624         concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 16, 16, 156)  0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 16, 16, 48)   7536        activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 16, 16, 48)   192         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 16, 16, 48)   0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 16, 16, 12)   5196        activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 16, 16, 168)  0           concatenate_20[0][0]             \n",
            "                                                                 conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 16, 16, 168)  672         concatenate_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 16, 16, 168)  0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 16, 16, 48)   8112        activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 16, 16, 48)   192         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 16, 16, 48)   0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 16, 16, 12)   5196        activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 16, 16, 180)  0           concatenate_21[0][0]             \n",
            "                                                                 conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 16, 16, 180)  720         concatenate_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 16, 16, 180)  0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 16, 16, 48)   8688        activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 16, 16, 48)   192         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 16, 16, 48)   0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 16, 16, 12)   5196        activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 16, 16, 192)  0           concatenate_22[0][0]             \n",
            "                                                                 conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 16, 16, 192)  768         concatenate_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 16, 16, 192)  0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 16, 16, 48)   9264        activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 16, 16, 48)   192         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 16, 16, 48)   0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 16, 16, 12)   5196        activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_24 (Concatenate)    (None, 16, 16, 204)  0           concatenate_23[0][0]             \n",
            "                                                                 conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 16, 16, 204)  816         concatenate_24[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 16, 16, 204)  0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 16, 16, 48)   9840        activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 16, 16, 48)   192         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 16, 16, 48)   0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 16, 16, 12)   5196        activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_25 (Concatenate)    (None, 16, 16, 216)  0           concatenate_24[0][0]             \n",
            "                                                                 conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 16, 16, 216)  864         concatenate_25[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 16, 16, 216)  0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 16, 16, 48)   10416       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 16, 16, 48)   192         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 16, 16, 48)   0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 16, 16, 12)   5196        activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_26 (Concatenate)    (None, 16, 16, 228)  0           concatenate_25[0][0]             \n",
            "                                                                 conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 16, 16, 228)  912         concatenate_26[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 16, 16, 228)  0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 16, 16, 48)   10992       activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 16, 16, 48)   192         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 16, 16, 48)   0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 16, 16, 12)   5196        activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_27 (Concatenate)    (None, 16, 16, 240)  0           concatenate_26[0][0]             \n",
            "                                                                 conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 16, 16, 240)  960         concatenate_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 16, 16, 240)  0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 16, 16, 48)   11568       activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 16, 16, 48)   192         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 16, 16, 48)   0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 16, 16, 12)   5196        activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_28 (Concatenate)    (None, 16, 16, 252)  0           concatenate_27[0][0]             \n",
            "                                                                 conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 16, 16, 252)  1008        concatenate_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 16, 16, 252)  0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 16, 16, 48)   12144       activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 16, 16, 48)   192         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 16, 16, 48)   0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 16, 16, 12)   5196        activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_29 (Concatenate)    (None, 16, 16, 264)  0           concatenate_28[0][0]             \n",
            "                                                                 conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 16, 16, 264)  1056        concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 16, 16, 264)  0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 16, 16, 48)   12720       activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 16, 16, 48)   192         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 16, 16, 48)   0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 16, 16, 12)   5196        activation_60[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_30 (Concatenate)    (None, 16, 16, 276)  0           concatenate_29[0][0]             \n",
            "                                                                 conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 16, 16, 276)  1104        concatenate_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 16, 16, 276)  0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 16, 16, 48)   13296       activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 16, 16, 48)   192         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 16, 16, 48)   0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 16, 16, 12)   5196        activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_31 (Concatenate)    (None, 16, 16, 288)  0           concatenate_30[0][0]             \n",
            "                                                                 conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 16, 16, 288)  1152        concatenate_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 16, 16, 288)  0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 16, 16, 48)   13872       activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 16, 16, 48)   192         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 16, 16, 48)   0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 16, 16, 12)   5196        activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_32 (Concatenate)    (None, 16, 16, 300)  0           concatenate_31[0][0]             \n",
            "                                                                 conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 16, 16, 300)  1200        concatenate_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 16, 16, 150)  45150       batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 8, 8, 150)    0           conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 8, 8, 150)    600         average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 8, 8, 150)    0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 8, 8, 48)     7248        activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 8, 8, 48)     192         conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 8, 8, 48)     0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 8, 8, 12)     5196        activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_33 (Concatenate)    (None, 8, 8, 162)    0           average_pooling2d_1[0][0]        \n",
            "                                                                 conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 8, 8, 162)    648         concatenate_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 8, 8, 162)    0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 8, 8, 48)     7824        activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 8, 8, 48)     192         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 8, 8, 48)     0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 8, 8, 12)     5196        activation_68[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_34 (Concatenate)    (None, 8, 8, 174)    0           concatenate_33[0][0]             \n",
            "                                                                 conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 8, 8, 174)    696         concatenate_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 8, 8, 174)    0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 8, 8, 48)     8400        activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 8, 8, 48)     192         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 8, 8, 48)     0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 8, 8, 12)     5196        activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_35 (Concatenate)    (None, 8, 8, 186)    0           concatenate_34[0][0]             \n",
            "                                                                 conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 8, 8, 186)    744         concatenate_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 8, 8, 186)    0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 8, 8, 48)     8976        activation_71[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 8, 8, 48)     192         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 8, 8, 48)     0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 8, 8, 12)     5196        activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_36 (Concatenate)    (None, 8, 8, 198)    0           concatenate_35[0][0]             \n",
            "                                                                 conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 8, 8, 198)    792         concatenate_36[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 8, 8, 198)    0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 8, 8, 48)     9552        activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 8, 8, 48)     192         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 8, 8, 48)     0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 8, 8, 12)     5196        activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_37 (Concatenate)    (None, 8, 8, 210)    0           concatenate_36[0][0]             \n",
            "                                                                 conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 8, 8, 210)    840         concatenate_37[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 8, 8, 210)    0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 8, 8, 48)     10128       activation_75[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 8, 8, 48)     192         conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 8, 8, 48)     0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 8, 8, 12)     5196        activation_76[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_38 (Concatenate)    (None, 8, 8, 222)    0           concatenate_37[0][0]             \n",
            "                                                                 conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 8, 8, 222)    888         concatenate_38[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 8, 8, 222)    0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 8, 8, 48)     10704       activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 8, 8, 48)     192         conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 8, 8, 48)     0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 8, 8, 12)     5196        activation_78[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_39 (Concatenate)    (None, 8, 8, 234)    0           concatenate_38[0][0]             \n",
            "                                                                 conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 8, 8, 234)    936         concatenate_39[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 8, 8, 234)    0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 8, 8, 48)     11280       activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 8, 8, 48)     192         conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 8, 8, 48)     0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 8, 8, 12)     5196        activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_40 (Concatenate)    (None, 8, 8, 246)    0           concatenate_39[0][0]             \n",
            "                                                                 conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 8, 8, 246)    984         concatenate_40[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 8, 8, 246)    0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 8, 8, 48)     11856       activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 8, 8, 48)     192         conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 8, 8, 48)     0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 8, 8, 12)     5196        activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_41 (Concatenate)    (None, 8, 8, 258)    0           concatenate_40[0][0]             \n",
            "                                                                 conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 8, 8, 258)    1032        concatenate_41[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 8, 8, 258)    0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 8, 8, 48)     12432       activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, 8, 8, 48)     192         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 8, 8, 48)     0           batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 8, 8, 12)     5196        activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_42 (Concatenate)    (None, 8, 8, 270)    0           concatenate_41[0][0]             \n",
            "                                                                 conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, 8, 8, 270)    1080        concatenate_42[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 8, 8, 270)    0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 8, 8, 48)     13008       activation_85[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, 8, 8, 48)     192         conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 8, 8, 48)     0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 8, 8, 12)     5196        activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_43 (Concatenate)    (None, 8, 8, 282)    0           concatenate_42[0][0]             \n",
            "                                                                 conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, 8, 8, 282)    1128        concatenate_43[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 8, 8, 282)    0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 8, 8, 48)     13584       activation_87[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, 8, 8, 48)     192         conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 8, 8, 48)     0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 8, 8, 12)     5196        activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_44 (Concatenate)    (None, 8, 8, 294)    0           concatenate_43[0][0]             \n",
            "                                                                 conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, 8, 8, 294)    1176        concatenate_44[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 8, 8, 294)    0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 8, 8, 48)     14160       activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, 8, 8, 48)     192         conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 8, 8, 48)     0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 8, 8, 12)     5196        activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_45 (Concatenate)    (None, 8, 8, 306)    0           concatenate_44[0][0]             \n",
            "                                                                 conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, 8, 8, 306)    1224        concatenate_45[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 8, 8, 306)    0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 8, 8, 48)     14736       activation_91[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_94 (BatchNo (None, 8, 8, 48)     192         conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 8, 8, 48)     0           batch_normalization_94[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 8, 8, 12)     5196        activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_46 (Concatenate)    (None, 8, 8, 318)    0           concatenate_45[0][0]             \n",
            "                                                                 conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_95 (BatchNo (None, 8, 8, 318)    1272        concatenate_46[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 8, 8, 318)    0           batch_normalization_95[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 8, 8, 48)     15312       activation_93[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_96 (BatchNo (None, 8, 8, 48)     192         conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_94 (Activation)      (None, 8, 8, 48)     0           batch_normalization_96[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 8, 8, 12)     5196        activation_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_47 (Concatenate)    (None, 8, 8, 330)    0           concatenate_46[0][0]             \n",
            "                                                                 conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_97 (BatchNo (None, 8, 8, 330)    1320        concatenate_47[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_95 (Activation)      (None, 8, 8, 330)    0           batch_normalization_97[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, 8, 8, 48)     15888       activation_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_98 (BatchNo (None, 8, 8, 48)     192         conv2d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_96 (Activation)      (None, 8, 8, 48)     0           batch_normalization_98[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 8, 8, 12)     5196        activation_96[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_48 (Concatenate)    (None, 8, 8, 342)    0           concatenate_47[0][0]             \n",
            "                                                                 conv2d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 1, 1, 342)    0           concatenate_48[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, 1, 1, 10)     3430        average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 10)           0           conv2d_99[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 797,788\n",
            "Trainable params: 774,376\n",
            "Non-trainable params: 23,412\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKWrjky_nZZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import time\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "from tensorflow.python.keras.callbacks import TensorBoard\n",
        "\n",
        "filepath = \"weights_best.hdf5\"\n",
        "history = tf.keras.callbacks.History()\n",
        "\n",
        "# tensorboard\n",
        "tensorboard = TensorBoard(log_dir=\"model_logs/{}\".format(time()))\n",
        "\n",
        "learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', \n",
        "                                            patience=3, \n",
        "                                            verbose=1, \n",
        "                                            factor=0.3, \n",
        "                                            min_lr=0.5e-6)\n",
        "checkpoint_save = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=1, mode='auto', baseline=None, restore_best_weights=False)\n",
        "\n",
        "callbacks_list = [checkpoint_save,learning_rate_reduction,history,tensorboard]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XosI7Ltx9Hah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "# create data generator\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in the range (deg 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally\n",
        "        height_shift_range=0.1,  # randomly shift images vertically\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "datagen.fit(X_train)       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtG8IUOa9XHo",
        "colab_type": "code",
        "outputId": "bf8d4fc6-3761-4dcb-aef4-93df0a3a82d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# fit model\n",
        "batch_size = 32\n",
        "steps = X_train.shape[0]//batch_size\n",
        "\n",
        "\n",
        "history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size), steps_per_epoch=steps, epochs=epochs, validation_data=(X_cv,y_cv), verbose=1, callbacks=callbacks_list)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 1.5886 - acc: 0.4242Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 10s 1ms/sample - loss: 1.2766 - acc: 0.4877\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.48770, saving model to weights_best.hdf5\n",
            "1250/1250 [==============================] - 216s 173ms/step - loss: 1.5884 - acc: 0.4242 - val_loss: 1.5196 - val_acc: 0.4877\n",
            "Epoch 2/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 1.1053 - acc: 0.6065Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 811us/sample - loss: 1.7057 - acc: 0.5606\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.48770 to 0.56060, saving model to weights_best.hdf5\n",
            "1250/1250 [==============================] - 133s 106ms/step - loss: 1.1052 - acc: 0.6065 - val_loss: 1.3950 - val_acc: 0.5606\n",
            "Epoch 3/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.8369 - acc: 0.7072Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 813us/sample - loss: 1.9630 - acc: 0.4279\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.56060\n",
            "1250/1250 [==============================] - 132s 105ms/step - loss: 0.8367 - acc: 0.7072 - val_loss: 2.1575 - val_acc: 0.4279\n",
            "Epoch 4/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.7004 - acc: 0.7580Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 848us/sample - loss: 0.5271 - acc: 0.7317\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.56060 to 0.73170, saving model to weights_best.hdf5\n",
            "1250/1250 [==============================] - 132s 106ms/step - loss: 0.7002 - acc: 0.7581 - val_loss: 0.8564 - val_acc: 0.7317\n",
            "Epoch 5/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.6124 - acc: 0.7893Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 831us/sample - loss: 1.1441 - acc: 0.7018\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.73170\n",
            "1250/1250 [==============================] - 135s 108ms/step - loss: 0.6122 - acc: 0.7894 - val_loss: 0.9417 - val_acc: 0.7018\n",
            "Epoch 6/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.5507 - acc: 0.8102Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 811us/sample - loss: 0.5197 - acc: 0.7902\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.73170 to 0.79020, saving model to weights_best.hdf5\n",
            "1250/1250 [==============================] - 133s 106ms/step - loss: 0.5506 - acc: 0.8103 - val_loss: 0.6464 - val_acc: 0.7902\n",
            "Epoch 7/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.5070 - acc: 0.8252Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 802us/sample - loss: 0.7736 - acc: 0.7495\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.79020\n",
            "1250/1250 [==============================] - 130s 104ms/step - loss: 0.5070 - acc: 0.8253 - val_loss: 0.8420 - val_acc: 0.7495\n",
            "Epoch 8/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4659 - acc: 0.8395Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 801us/sample - loss: 0.3874 - acc: 0.8147\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.79020 to 0.81470, saving model to weights_best.hdf5\n",
            "1250/1250 [==============================] - 131s 105ms/step - loss: 0.4658 - acc: 0.8396 - val_loss: 0.5656 - val_acc: 0.8147\n",
            "Epoch 9/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4342 - acc: 0.8503Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 805us/sample - loss: 0.5362 - acc: 0.7905\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.81470\n",
            "1250/1250 [==============================] - 129s 103ms/step - loss: 0.4341 - acc: 0.8504 - val_loss: 0.6573 - val_acc: 0.7905\n",
            "Epoch 10/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4080 - acc: 0.8596Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 799us/sample - loss: 0.4543 - acc: 0.8040\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.81470\n",
            "1250/1250 [==============================] - 129s 103ms/step - loss: 0.4080 - acc: 0.8596 - val_loss: 0.6447 - val_acc: 0.8040\n",
            "Epoch 11/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.3855 - acc: 0.8675Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 804us/sample - loss: 0.4556 - acc: 0.8169\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.81470 to 0.81690, saving model to weights_best.hdf5\n",
            "1250/1250 [==============================] - 130s 104ms/step - loss: 0.3855 - acc: 0.8675 - val_loss: 0.5810 - val_acc: 0.8169\n",
            "Epoch 12/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.3611 - acc: 0.8747Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 802us/sample - loss: 0.4681 - acc: 0.8193\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.81690 to 0.81930, saving model to weights_best.hdf5\n",
            "1250/1250 [==============================] - 130s 104ms/step - loss: 0.3610 - acc: 0.8747 - val_loss: 0.5517 - val_acc: 0.8193\n",
            "Epoch 13/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.3445 - acc: 0.8810Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 796us/sample - loss: 0.2779 - acc: 0.8603\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.81930 to 0.86030, saving model to weights_best.hdf5\n",
            "1250/1250 [==============================] - 130s 104ms/step - loss: 0.3445 - acc: 0.8810 - val_loss: 0.4415 - val_acc: 0.8603\n",
            "Epoch 14/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.3255 - acc: 0.8865Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 792us/sample - loss: 0.6262 - acc: 0.8282\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.86030\n",
            "1250/1250 [==============================] - 128s 103ms/step - loss: 0.3256 - acc: 0.8865 - val_loss: 0.5944 - val_acc: 0.8282\n",
            "Epoch 15/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.3096 - acc: 0.8932Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 796us/sample - loss: 0.3674 - acc: 0.8606\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.86030 to 0.86060, saving model to weights_best.hdf5\n",
            "1250/1250 [==============================] - 129s 103ms/step - loss: 0.3096 - acc: 0.8932 - val_loss: 0.4187 - val_acc: 0.8606\n",
            "Epoch 16/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.2958 - acc: 0.8985Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 793us/sample - loss: 0.3241 - acc: 0.8440\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.86060\n",
            "1250/1250 [==============================] - 128s 102ms/step - loss: 0.2957 - acc: 0.8985 - val_loss: 0.5020 - val_acc: 0.8440\n",
            "Epoch 17/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.2865 - acc: 0.9006Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 796us/sample - loss: 0.4609 - acc: 0.8545\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.86060\n",
            "1250/1250 [==============================] - 128s 103ms/step - loss: 0.2864 - acc: 0.9007 - val_loss: 0.4768 - val_acc: 0.8545\n",
            "Epoch 18/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.2683 - acc: 0.9065Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 825us/sample - loss: 0.5480 - acc: 0.8445\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.86060\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "1250/1250 [==============================] - 130s 104ms/step - loss: 0.2682 - acc: 0.9065 - val_loss: 0.5646 - val_acc: 0.8445\n",
            "Epoch 19/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.1785 - acc: 0.9387Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 836us/sample - loss: 0.2518 - acc: 0.9031\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.86060 to 0.90310, saving model to weights_best.hdf5\n",
            "1250/1250 [==============================] - 134s 107ms/step - loss: 0.1785 - acc: 0.9386 - val_loss: 0.3101 - val_acc: 0.9031\n",
            "Epoch 20/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.1545 - acc: 0.9465Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 797us/sample - loss: 0.4257 - acc: 0.8958\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.90310\n",
            "1250/1250 [==============================] - 129s 103ms/step - loss: 0.1544 - acc: 0.9465 - val_loss: 0.3444 - val_acc: 0.8958\n",
            "Epoch 21/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.9519Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 801us/sample - loss: 0.2080 - acc: 0.8988\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.90310\n",
            "1250/1250 [==============================] - 128s 103ms/step - loss: 0.1374 - acc: 0.9519 - val_loss: 0.3427 - val_acc: 0.8988\n",
            "Epoch 22/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.1387 - acc: 0.9524Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 801us/sample - loss: 0.2061 - acc: 0.8999\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.90310\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "1250/1250 [==============================] - 128s 103ms/step - loss: 0.1387 - acc: 0.9524 - val_loss: 0.3398 - val_acc: 0.8999\n",
            "Epoch 23/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9628Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 800us/sample - loss: 0.3078 - acc: 0.9139\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.90310 to 0.91390, saving model to weights_best.hdf5\n",
            "1250/1250 [==============================] - 130s 104ms/step - loss: 0.1076 - acc: 0.9628 - val_loss: 0.2818 - val_acc: 0.9139\n",
            "Epoch 24/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0998 - acc: 0.9657Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 795us/sample - loss: 0.2581 - acc: 0.9138\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.91390\n",
            "1250/1250 [==============================] - 128s 102ms/step - loss: 0.0997 - acc: 0.9657 - val_loss: 0.2881 - val_acc: 0.9138\n",
            "Epoch 25/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0941 - acc: 0.9676Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 792us/sample - loss: 0.2160 - acc: 0.9149\n",
            "\n",
            "Epoch 00025: val_acc improved from 0.91390 to 0.91490, saving model to weights_best.hdf5\n",
            "1250/1250 [==============================] - 129s 103ms/step - loss: 0.0941 - acc: 0.9676 - val_loss: 0.2872 - val_acc: 0.9149\n",
            "Epoch 26/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0920 - acc: 0.9677Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 796us/sample - loss: 0.2664 - acc: 0.9142\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.91490\n",
            "1250/1250 [==============================] - 128s 102ms/step - loss: 0.0919 - acc: 0.9677 - val_loss: 0.2932 - val_acc: 0.9142\n",
            "Epoch 27/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9705Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 795us/sample - loss: 0.2370 - acc: 0.9136\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.91490\n",
            "1250/1250 [==============================] - 128s 103ms/step - loss: 0.0861 - acc: 0.9705 - val_loss: 0.2985 - val_acc: 0.9136\n",
            "Epoch 28/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9718Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 789us/sample - loss: 0.2494 - acc: 0.9145\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.91490\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "1250/1250 [==============================] - 128s 102ms/step - loss: 0.0807 - acc: 0.9718 - val_loss: 0.3102 - val_acc: 0.9145\n",
            "Epoch 29/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9732Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 793us/sample - loss: 0.2405 - acc: 0.9159\n",
            "\n",
            "Epoch 00029: val_acc improved from 0.91490 to 0.91590, saving model to weights_best.hdf5\n",
            "1250/1250 [==============================] - 129s 103ms/step - loss: 0.0770 - acc: 0.9732 - val_loss: 0.2925 - val_acc: 0.9159\n",
            "Epoch 30/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9752Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 787us/sample - loss: 0.2632 - acc: 0.9160\n",
            "\n",
            "Epoch 00030: val_acc improved from 0.91590 to 0.91600, saving model to weights_best.hdf5\n",
            "1250/1250 [==============================] - 128s 103ms/step - loss: 0.0716 - acc: 0.9752 - val_loss: 0.2983 - val_acc: 0.9160\n",
            "Epoch 31/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9752Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 789us/sample - loss: 0.1995 - acc: 0.9169\n",
            "\n",
            "Epoch 00031: val_acc improved from 0.91600 to 0.91690, saving model to weights_best.hdf5\n",
            "1250/1250 [==============================] - 128s 103ms/step - loss: 0.0724 - acc: 0.9752 - val_loss: 0.2954 - val_acc: 0.9169\n",
            "Epoch 32/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0700 - acc: 0.9760Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 794us/sample - loss: 0.2074 - acc: 0.9177\n",
            "\n",
            "Epoch 00032: val_acc improved from 0.91690 to 0.91770, saving model to weights_best.hdf5\n",
            "1250/1250 [==============================] - 128s 102ms/step - loss: 0.0700 - acc: 0.9760 - val_loss: 0.2955 - val_acc: 0.9177\n",
            "Epoch 33/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9765Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 814us/sample - loss: 0.2170 - acc: 0.9169\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.91770\n",
            "1250/1250 [==============================] - 131s 105ms/step - loss: 0.0689 - acc: 0.9765 - val_loss: 0.3024 - val_acc: 0.9169\n",
            "Epoch 34/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9768Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 789us/sample - loss: 0.2454 - acc: 0.9189\n",
            "\n",
            "Epoch 00034: val_acc improved from 0.91770 to 0.91890, saving model to weights_best.hdf5\n",
            "1250/1250 [==============================] - 129s 103ms/step - loss: 0.0692 - acc: 0.9768 - val_loss: 0.3004 - val_acc: 0.9189\n",
            "Epoch 35/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9777Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 791us/sample - loss: 0.2411 - acc: 0.9173\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.91890\n",
            "1250/1250 [==============================] - 126s 101ms/step - loss: 0.0664 - acc: 0.9777 - val_loss: 0.3032 - val_acc: 0.9173\n",
            "Epoch 36/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9764Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 801us/sample - loss: 0.2418 - acc: 0.9178\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.91890\n",
            "1250/1250 [==============================] - 127s 101ms/step - loss: 0.0667 - acc: 0.9764 - val_loss: 0.3022 - val_acc: 0.9178\n",
            "Epoch 37/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9768Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 788us/sample - loss: 0.2301 - acc: 0.9187\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.91890\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 8.100000013655517e-06.\n",
            "1250/1250 [==============================] - 126s 101ms/step - loss: 0.0667 - acc: 0.9768 - val_loss: 0.3074 - val_acc: 0.9187\n",
            "Epoch 38/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9783Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 794us/sample - loss: 0.2301 - acc: 0.9188\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.91890\n",
            "1250/1250 [==============================] - 127s 101ms/step - loss: 0.0640 - acc: 0.9783 - val_loss: 0.3028 - val_acc: 0.9188\n",
            "Epoch 39/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9779Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 789us/sample - loss: 0.2324 - acc: 0.9183\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.91890\n",
            "1250/1250 [==============================] - 127s 101ms/step - loss: 0.0631 - acc: 0.9779 - val_loss: 0.3019 - val_acc: 0.9183\n",
            "Epoch 40/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9770Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 791us/sample - loss: 0.2369 - acc: 0.9182\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.91890\n",
            "\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 2.429999949526973e-06.\n",
            "1250/1250 [==============================] - 127s 102ms/step - loss: 0.0664 - acc: 0.9770 - val_loss: 0.3023 - val_acc: 0.9182\n",
            "Epoch 41/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9802Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 789us/sample - loss: 0.2370 - acc: 0.9184\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.91890\n",
            "1250/1250 [==============================] - 127s 101ms/step - loss: 0.0587 - acc: 0.9801 - val_loss: 0.3045 - val_acc: 0.9184\n",
            "Epoch 42/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9787Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 790us/sample - loss: 0.2386 - acc: 0.9185\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.91890\n",
            "1250/1250 [==============================] - 127s 102ms/step - loss: 0.0621 - acc: 0.9787 - val_loss: 0.3051 - val_acc: 0.9185\n",
            "Epoch 43/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9785Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 810us/sample - loss: 0.2386 - acc: 0.9190\n",
            "\n",
            "Epoch 00043: val_acc improved from 0.91890 to 0.91900, saving model to weights_best.hdf5\n",
            "1250/1250 [==============================] - 128s 102ms/step - loss: 0.0616 - acc: 0.9785 - val_loss: 0.3046 - val_acc: 0.9190\n",
            "Epoch 44/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9798Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 789us/sample - loss: 0.2344 - acc: 0.9182\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 127s 102ms/step - loss: 0.0592 - acc: 0.9797 - val_loss: 0.3044 - val_acc: 0.9182\n",
            "Epoch 45/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9791Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 794us/sample - loss: 0.2356 - acc: 0.9184\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 127s 101ms/step - loss: 0.0616 - acc: 0.9791 - val_loss: 0.3037 - val_acc: 0.9184\n",
            "Epoch 46/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9789Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 790us/sample - loss: 0.2343 - acc: 0.9184\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.91900\n",
            "\n",
            "Epoch 00046: ReduceLROnPlateau reducing learning rate to 7.289999985005124e-07.\n",
            "1250/1250 [==============================] - 127s 102ms/step - loss: 0.0606 - acc: 0.9789 - val_loss: 0.3037 - val_acc: 0.9184\n",
            "Epoch 47/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9791Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 815us/sample - loss: 0.2306 - acc: 0.9182\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 132s 105ms/step - loss: 0.0607 - acc: 0.9790 - val_loss: 0.3025 - val_acc: 0.9182\n",
            "Epoch 48/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9794Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 794us/sample - loss: 0.2360 - acc: 0.9178\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 128s 103ms/step - loss: 0.0606 - acc: 0.9793 - val_loss: 0.3040 - val_acc: 0.9178\n",
            "Epoch 49/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9788Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 786us/sample - loss: 0.2220 - acc: 0.9189\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.91900\n",
            "\n",
            "Epoch 00049: ReduceLROnPlateau reducing learning rate to 5e-07.\n",
            "1250/1250 [==============================] - 127s 101ms/step - loss: 0.0609 - acc: 0.9789 - val_loss: 0.3019 - val_acc: 0.9189\n",
            "Epoch 50/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9799Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 785us/sample - loss: 0.2328 - acc: 0.9186\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 127s 102ms/step - loss: 0.0588 - acc: 0.9798 - val_loss: 0.3025 - val_acc: 0.9186\n",
            "Epoch 51/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9789Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 789us/sample - loss: 0.2298 - acc: 0.9178\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 127s 102ms/step - loss: 0.0621 - acc: 0.9789 - val_loss: 0.3022 - val_acc: 0.9178\n",
            "Epoch 52/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9799Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 790us/sample - loss: 0.2242 - acc: 0.9177\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 126s 101ms/step - loss: 0.0600 - acc: 0.9799 - val_loss: 0.3028 - val_acc: 0.9177\n",
            "Epoch 53/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9791Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 793us/sample - loss: 0.2180 - acc: 0.9178\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 127s 102ms/step - loss: 0.0604 - acc: 0.9791 - val_loss: 0.3014 - val_acc: 0.9178\n",
            "Epoch 54/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9797Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 794us/sample - loss: 0.2303 - acc: 0.9183\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 127s 102ms/step - loss: 0.0602 - acc: 0.9797 - val_loss: 0.3017 - val_acc: 0.9183\n",
            "Epoch 55/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9796Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 786us/sample - loss: 0.2298 - acc: 0.9181\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 127s 102ms/step - loss: 0.0611 - acc: 0.9797 - val_loss: 0.3041 - val_acc: 0.9181\n",
            "Epoch 56/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9788Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 785us/sample - loss: 0.2310 - acc: 0.9186\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 127s 102ms/step - loss: 0.0613 - acc: 0.9789 - val_loss: 0.3024 - val_acc: 0.9186\n",
            "Epoch 57/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9788Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 785us/sample - loss: 0.2235 - acc: 0.9184\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 127s 102ms/step - loss: 0.0622 - acc: 0.9788 - val_loss: 0.3017 - val_acc: 0.9184\n",
            "Epoch 58/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9791Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 793us/sample - loss: 0.2338 - acc: 0.9185\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 126s 101ms/step - loss: 0.0582 - acc: 0.9791 - val_loss: 0.3029 - val_acc: 0.9185\n",
            "Epoch 59/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9784Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 798us/sample - loss: 0.2338 - acc: 0.9171\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 129s 103ms/step - loss: 0.0616 - acc: 0.9785 - val_loss: 0.3035 - val_acc: 0.9171\n",
            "Epoch 60/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9789Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 813us/sample - loss: 0.2351 - acc: 0.9179\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 127s 102ms/step - loss: 0.0610 - acc: 0.9789 - val_loss: 0.3049 - val_acc: 0.9179\n",
            "Epoch 61/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9803Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 841us/sample - loss: 0.2279 - acc: 0.9182\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 131s 105ms/step - loss: 0.0589 - acc: 0.9803 - val_loss: 0.3041 - val_acc: 0.9182\n",
            "Epoch 62/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9791Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 792us/sample - loss: 0.2276 - acc: 0.9179\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 131s 105ms/step - loss: 0.0602 - acc: 0.9790 - val_loss: 0.3031 - val_acc: 0.9179\n",
            "Epoch 63/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9797Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 792us/sample - loss: 0.2317 - acc: 0.9184\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 127s 102ms/step - loss: 0.0598 - acc: 0.9797 - val_loss: 0.3035 - val_acc: 0.9184\n",
            "Epoch 64/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9787Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 790us/sample - loss: 0.2229 - acc: 0.9185\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 128s 102ms/step - loss: 0.0606 - acc: 0.9786 - val_loss: 0.3026 - val_acc: 0.9185\n",
            "Epoch 65/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9799Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 798us/sample - loss: 0.2274 - acc: 0.9181\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 129s 103ms/step - loss: 0.0595 - acc: 0.9798 - val_loss: 0.3042 - val_acc: 0.9181\n",
            "Epoch 66/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9798Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 793us/sample - loss: 0.2275 - acc: 0.9184\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 128s 102ms/step - loss: 0.0606 - acc: 0.9797 - val_loss: 0.3028 - val_acc: 0.9184\n",
            "Epoch 67/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9792Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 792us/sample - loss: 0.2305 - acc: 0.9184\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 128s 103ms/step - loss: 0.0603 - acc: 0.9792 - val_loss: 0.3045 - val_acc: 0.9184\n",
            "Epoch 68/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9783Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 789us/sample - loss: 0.2315 - acc: 0.9187\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 128s 102ms/step - loss: 0.0614 - acc: 0.9783 - val_loss: 0.3025 - val_acc: 0.9187\n",
            "Epoch 69/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9792Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 796us/sample - loss: 0.2315 - acc: 0.9179\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 128s 103ms/step - loss: 0.0608 - acc: 0.9792 - val_loss: 0.3065 - val_acc: 0.9179\n",
            "Epoch 70/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9800Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 800us/sample - loss: 0.2284 - acc: 0.9182\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 129s 103ms/step - loss: 0.0590 - acc: 0.9800 - val_loss: 0.3027 - val_acc: 0.9182\n",
            "Epoch 71/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9779Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 796us/sample - loss: 0.2317 - acc: 0.9185\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 128s 103ms/step - loss: 0.0628 - acc: 0.9779 - val_loss: 0.3025 - val_acc: 0.9185\n",
            "Epoch 72/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9799Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 805us/sample - loss: 0.2345 - acc: 0.9188\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 129s 103ms/step - loss: 0.0594 - acc: 0.9799 - val_loss: 0.3023 - val_acc: 0.9188\n",
            "Epoch 73/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9796Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 806us/sample - loss: 0.2241 - acc: 0.9184\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 130s 104ms/step - loss: 0.0604 - acc: 0.9796 - val_loss: 0.3031 - val_acc: 0.9184\n",
            "Epoch 74/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9796Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 807us/sample - loss: 0.2416 - acc: 0.9180\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 130s 104ms/step - loss: 0.0592 - acc: 0.9796 - val_loss: 0.3039 - val_acc: 0.9180\n",
            "Epoch 75/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9789Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 9s 856us/sample - loss: 0.2288 - acc: 0.9185\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 134s 107ms/step - loss: 0.0612 - acc: 0.9789 - val_loss: 0.3036 - val_acc: 0.9185\n",
            "Epoch 76/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9779Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 796us/sample - loss: 0.2271 - acc: 0.9184\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 130s 104ms/step - loss: 0.0621 - acc: 0.9779 - val_loss: 0.3032 - val_acc: 0.9184\n",
            "Epoch 77/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9793Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 793us/sample - loss: 0.2367 - acc: 0.9186\n",
            "\n",
            "Epoch 00077: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 128s 103ms/step - loss: 0.0601 - acc: 0.9793 - val_loss: 0.3028 - val_acc: 0.9186\n",
            "Epoch 78/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9795Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 794us/sample - loss: 0.2276 - acc: 0.9189\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 127s 102ms/step - loss: 0.0597 - acc: 0.9795 - val_loss: 0.3022 - val_acc: 0.9189\n",
            "Epoch 79/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9798Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 794us/sample - loss: 0.2212 - acc: 0.9177\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 128s 103ms/step - loss: 0.0581 - acc: 0.9798 - val_loss: 0.3028 - val_acc: 0.9177\n",
            "Epoch 80/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9796Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 793us/sample - loss: 0.2291 - acc: 0.9179\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 128s 102ms/step - loss: 0.0593 - acc: 0.9796 - val_loss: 0.3035 - val_acc: 0.9179\n",
            "Epoch 81/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9798Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 787us/sample - loss: 0.2296 - acc: 0.9185\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 128s 102ms/step - loss: 0.0595 - acc: 0.9798 - val_loss: 0.3046 - val_acc: 0.9185\n",
            "Epoch 82/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9790Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 801us/sample - loss: 0.2354 - acc: 0.9180\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 128s 102ms/step - loss: 0.0601 - acc: 0.9790 - val_loss: 0.3039 - val_acc: 0.9180\n",
            "Epoch 83/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9782Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 798us/sample - loss: 0.2206 - acc: 0.9184\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 128s 103ms/step - loss: 0.0634 - acc: 0.9782 - val_loss: 0.3034 - val_acc: 0.9184\n",
            "Epoch 84/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9791Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 796us/sample - loss: 0.2290 - acc: 0.9187\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.91900\n",
            "1250/1250 [==============================] - 128s 102ms/step - loss: 0.0600 - acc: 0.9791 - val_loss: 0.3038 - val_acc: 0.9187\n",
            "Epoch 85/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0618 - acc: 0.9786Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 799us/sample - loss: 0.2316 - acc: 0.9192\n",
            "\n",
            "Epoch 00085: val_acc improved from 0.91900 to 0.91920, saving model to weights_best.hdf5\n",
            "1250/1250 [==============================] - 129s 103ms/step - loss: 0.0619 - acc: 0.9785 - val_loss: 0.3032 - val_acc: 0.9192\n",
            "Epoch 86/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9800Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 798us/sample - loss: 0.2302 - acc: 0.9173\n",
            "\n",
            "Epoch 00086: val_acc did not improve from 0.91920\n",
            "1250/1250 [==============================] - 128s 102ms/step - loss: 0.0603 - acc: 0.9800 - val_loss: 0.3050 - val_acc: 0.9173\n",
            "Epoch 87/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9794Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 795us/sample - loss: 0.2370 - acc: 0.9185\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.91920\n",
            "1250/1250 [==============================] - 128s 102ms/step - loss: 0.0588 - acc: 0.9794 - val_loss: 0.3045 - val_acc: 0.9185\n",
            "Epoch 88/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9781Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 796us/sample - loss: 0.2338 - acc: 0.9186\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.91920\n",
            "1250/1250 [==============================] - 128s 103ms/step - loss: 0.0606 - acc: 0.9781 - val_loss: 0.3037 - val_acc: 0.9186\n",
            "Epoch 89/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9790Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 818us/sample - loss: 0.2279 - acc: 0.9183\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 0.91920\n",
            "1250/1250 [==============================] - 130s 104ms/step - loss: 0.0614 - acc: 0.9790 - val_loss: 0.3033 - val_acc: 0.9183\n",
            "Epoch 90/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9801Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 795us/sample - loss: 0.2371 - acc: 0.9184\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.91920\n",
            "1250/1250 [==============================] - 129s 103ms/step - loss: 0.0600 - acc: 0.9801 - val_loss: 0.3040 - val_acc: 0.9184\n",
            "Epoch 91/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9794Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 792us/sample - loss: 0.2329 - acc: 0.9186\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.91920\n",
            "1250/1250 [==============================] - 128s 103ms/step - loss: 0.0606 - acc: 0.9794 - val_loss: 0.3031 - val_acc: 0.9186\n",
            "Epoch 92/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9791Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 794us/sample - loss: 0.2316 - acc: 0.9193\n",
            "\n",
            "Epoch 00092: val_acc improved from 0.91920 to 0.91930, saving model to weights_best.hdf5\n",
            "1250/1250 [==============================] - 129s 103ms/step - loss: 0.0595 - acc: 0.9790 - val_loss: 0.3029 - val_acc: 0.9193\n",
            "Epoch 93/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9787Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 791us/sample - loss: 0.2270 - acc: 0.9193\n",
            "\n",
            "Epoch 00093: val_acc did not improve from 0.91930\n",
            "1250/1250 [==============================] - 128s 102ms/step - loss: 0.0616 - acc: 0.9787 - val_loss: 0.3010 - val_acc: 0.9193\n",
            "Epoch 94/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9795Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 787us/sample - loss: 0.2206 - acc: 0.9190\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 0.91930\n",
            "1250/1250 [==============================] - 128s 102ms/step - loss: 0.0588 - acc: 0.9796 - val_loss: 0.3028 - val_acc: 0.9190\n",
            "Epoch 95/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9793Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 801us/sample - loss: 0.2325 - acc: 0.9179\n",
            "\n",
            "Epoch 00095: val_acc did not improve from 0.91930\n",
            "1250/1250 [==============================] - 128s 102ms/step - loss: 0.0607 - acc: 0.9793 - val_loss: 0.3045 - val_acc: 0.9179\n",
            "Epoch 96/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9784Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 800us/sample - loss: 0.2271 - acc: 0.9185\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.91930\n",
            "1250/1250 [==============================] - 128s 102ms/step - loss: 0.0611 - acc: 0.9784 - val_loss: 0.3031 - val_acc: 0.9185\n",
            "Epoch 97/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9794Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 789us/sample - loss: 0.2356 - acc: 0.9187\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.91930\n",
            "1250/1250 [==============================] - 128s 102ms/step - loss: 0.0610 - acc: 0.9794 - val_loss: 0.3032 - val_acc: 0.9187\n",
            "Epoch 98/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9794Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 791us/sample - loss: 0.2341 - acc: 0.9185\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.91930\n",
            "1250/1250 [==============================] - 127s 102ms/step - loss: 0.0595 - acc: 0.9794 - val_loss: 0.3046 - val_acc: 0.9185\n",
            "Epoch 99/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9794Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 793us/sample - loss: 0.2351 - acc: 0.9180\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.91930\n",
            "1250/1250 [==============================] - 128s 102ms/step - loss: 0.0621 - acc: 0.9794 - val_loss: 0.3043 - val_acc: 0.9180\n",
            "Epoch 100/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9790Epoch 1/100\n",
            "10000/1250 [================================================================================================================================================================================================================================================] - 8s 792us/sample - loss: 0.2263 - acc: 0.9188\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.91930\n",
            "1250/1250 [==============================] - 127s 101ms/step - loss: 0.0588 - acc: 0.9790 - val_loss: 0.3021 - val_acc: 0.9188\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZcWydmIVhZGr",
        "outputId": "ef7942ce-3aa7-43c8-b45f-d4e14635cf0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Test the model\n",
        "model.load_weights('weights_best.hdf5')\n",
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 7s 681us/sample - loss: 0.3245 - acc: 0.9127\n",
            "Test loss: 0.3244841086283326\n",
            "Test accuracy: 0.9127\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eDKGZYcLpKH",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "*   At epoch 93 I recieved the validation accuracy of 91.93 with the optimizer Adam which works fine. I built the model on keras framework with data augmentation.\n",
        "*   The test accuracy is 91.27% on 10,000 datapoints with 0.331 Test loss.\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}